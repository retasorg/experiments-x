# Preprocessed Data Directory

This directory contains preprocessed training/test data generated by `1_preprocess_data_FIXED.py`.

## Files Generated:

### NumPy Arrays (Preprocessed Features & Labels):
- **X_train.npy** - Training features (normalized, one-hot encoded)
- **y_train.npy** - Training labels (0=Normal, 1=Attack)
- **X_test.npy** - Test features (normalized, one-hot encoded)
- **y_test.npy** - Test labels (0=Normal, 1=Attack)
- **X_normal.npy** - Normal traffic only (for Isolation Forest training)

### Preprocessing Artifacts:
- **scaler.pkl** - StandardScaler for inference (IMPORTANT: needed for production!)
- **feature_names.txt** - List of feature names after one-hot encoding

## File Sizes (Approximate):
- X_train.npy: ~200-300 MB
- X_test.npy: ~100-150 MB
- X_normal.npy: ~100-150 MB
- Others: < 1 MB

## Usage:

### For Training:
```python
import numpy as np
X_train = np.load('./data/X_train.npy')
y_train = np.load('./data/y_train.npy')
```

### For Inference (Production):
```python
import pickle
with open('./data/scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

# Scale new data the same way as training
new_data_scaled = scaler.transform(new_data)
```

## Important Notes:
- This data is **already normalized** - don't normalize again!
- Features are **one-hot encoded** (~100-200 features after encoding)
- **scaler.pkl** must be deployed with your models for production inference
- Don't commit large .npy files to git (add to .gitignore)

## Regenerating:
If you need to regenerate this data:
```bash
python 1_preprocess_data_FIXED.py
# or with Docker:
docker compose up
```
